{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [2], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcsv\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscrapy\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "from asyncore import write\n",
    "from bs4 import BeautifulSoup \n",
    "import requests\n",
    "import csv\n",
    "import scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import library yang akan digunakan untuk ketiga program di bawah. Library write digunakan untuk membaca file menjadi csv (output akan dihasilkan di csv bukan di terminal)m kaku BeautifulSoup untuk membaca dan mengambil data HTML serta sebagai parser untuk memisahkan komponen HTML yang mudah dibaca, lalu requests untuk mengambil data dari laman website dengan mengirimkan request ke server situs web yang kita tuju untuk scraping, lau csv sebagai library untuk membatu mengkonversi tempat output, dan terakhir scrapy yakni bot dalam membantu proses scraping dan scrawling web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOMOR 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://unair.ac.id/news'\n",
    "respone = requests.get(url)\n",
    "rawhtml = respone.text\n",
    "soup = BeautifulSoup(rawhtml, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kita membaca laman link untuk scraping dengan cara request.get pada variabel link dituju. Setelah dengan variabel respone untuk membaca di rawhtml (format html yang dibaca dalam text). Tahap terakhir adalah dengan memanggil library BeautifulSoup pada variabel rawhtml dan setiap bagian dipisahkan atau diuraikan dengan parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\t\t\t\tKontribusi Terhadap Keberlanjutan Lingkungan, UNAIR Raih 4 Trees Rating dari UI Greenmetric\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\tFK UNAIR Sambut 31 PPDS Program Hybrid TNI\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\tDekan FH UNAIR: Pendidikan Hukum Harus Berubah Agar Dapat Mencetak Pemimpin Masa Depan\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\tJalani Hidup Sehat, Ini Yang Perlu DisiapkanÂ \t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\t5 Mahasiswa UNAIR Ikuti Student Exchange dan Konferensi di Thailand, Malaysia, dan Singapura\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\tDepartemen Komunikasi UNAIR Gelar Kuliah Tamu Kesehatan Mental Remaja\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\tWaspadai Resistensi Antibiotik Saat Sakit\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\tDosen Toksikologi UNAIR Paparkan Toksisitas Etilen Glikol, Senyawa Kimia yang Diduga Penyebab Gagal Ginjal Akut pada Anak\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\tDosen UNAIR Dorong Percepatan Kastrasi Jantan untuk Tekan Populasi Kucing Liar\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\tDua Mahasiswa Perikanan UNAIR Ikuti Summer School di Hiroshima University\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\tSambut Pertukaran Mahasiswa, Rektor UNAIR: Pulang Harus Jadi Orang Hebat\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\tMawapres FK UNAIR, Sebulan Borong 4 Gelar Juara\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\tMahasiswa Ilmu Sejarah UNAIR Dinobatkan sebagai Putri Pariwisata Kalimantan Timur 2022\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\tKajian Sejarah Pers Indonesia Bawa Mahasiswa FIB Sabet Juara 3 LKTIN di UI\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\tMahasiswa Farmasi UNAIR Cetuskan Ide Pengobatan Kanker Payudara Berbasis Daun Pepaya\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\tInovasi Dosen FPK UNAIR Manfaatkan Limbah PT. Ajinomoto Sebagai Pakan Ikan\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\tPeneliti FKM UNAIR Kembangkan Formula Fermentasi Bawang untuk Lansia Pasca Vaksinasi\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\tKetua KPK Kunjungi UNAIR\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\tUNAIR Terima Kunjungan Dubes Pakistan\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\tMoU UNAIR-Pemkab Banyuwangi Perkuat Kerjasama Bidang Pendidikan\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\tEfek Penggunaan Chlorhexidine dan Povidone-Iodine Terhadap viral load SARS-CoV-2\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\tScaffold Nanofiber Berbasis Polylacticacid-Polycaprolactone Untuk Cedera Anterior Cruciate Ligament\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\tAutograf Tulang Rawan dalam Penanganan Kasus Ameloblastoma Pasca Hemi-mandibulektomi\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\tMengenal Kanker Kepala-Leher yang Disebabkan oleh Virus HPV\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\tMahasiswa Belanda Sebar Bibit Ikan untuk Cegah Malaria di Banyuwangi\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\tDepartemen THT-KL Kukuhkan Ahli Onkologi THT dari Malaysia jadi Adjunct Professor\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\tMawapres FK UNAIR, Sebulan Borong 4 Gelar Juara\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\tKajian Sejarah Pers Indonesia Bawa Mahasiswa FIB Sabet Juara 3 LKTIN di UI\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\tGelisah Penggunaan Plastik di Kereta Api, Mahasiswa UNAIR Temukan Gerakan Anti Plastik\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\tGowes Bareng Gubernur dan Alumni UNAIR Ramaikan World Heart Day\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\tKhofifah Resmi Mulai Semarak Peringatan HUT EMAS IKA UNAIR\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\tAlumni UNAIR Siap Luncurkan Aplikasi untuk Petani Walet\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\tHari Santri, Warek SD UNAIR Jelaskan Peran Santri dalam Bidang Ekologi\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\tMenteri Hingga Penulis Hadir dalam Webinar Series Berskala Internasional HMD Sasindo UNAIR\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\tEkstrak Etanol Mangrove Gagal Menghambat Pertumbuhan Candida albicans Diisolasi dari Oral Kandidiasis Pasien HIV/AIDS in vitro\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\tRilis: Mahasiswa UNAIR Bagikan Angklung di Rusia\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\tSekolah Rodinda, Jurnalis Tirto Sampaikan Cara Mencegah Hoaks dalam Berita\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\tMahasiswa dan Dosen Kedokteran Hewan SIKIA Ungkap Peluang Teripang Laut dan Sisik Ikan Nila sebagai Material Tendon Buatan\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\tKarakteristik Membran Amnion sebagai Kandidat Bahan Terapi di Bidang Kedokteran Gigi\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\tPerbandingan Deformasi Permanen Kawat Termal Nikel-Titanium 0,014 Menggunakan IMD Orthoshaped, American Orthodontics, dan Ormco\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\tLumbar Disc Herniation pada Remaja Perempuan Berusia 15 Tahun\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\tTim UNAIR Sabet 3 Penghargaan di Ajang Lomba Debat Nasional KITA Brawijaya\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\tMahasiswa FKp UNAIR Sabet Juara I Cerdas Cermat NSF Brawijaya\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\tBLS FH UNAIR Ungkap Karakteristik Perseroan Perseorangan dan Problematikanya\t\t\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in soup.find_all('h3'):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dikarenakan saya ingin mengetahui apa saja judul yang digunakan pada laman pertama, dengan library soup saya membaca setiap h3 (adapun h3 adalah tag yang digunakan sebagai judul di laman website terkait), looping (for) digunakan untuk melakukan scraping tiap- tiap baris yang memuat tag h3 (judul). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nomor 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = []\n",
    "maxpage = 20\n",
    "for page in range(1,maxpage+1):\n",
    "    url = 'https://www.unair.ac.id/category/pakar/page/{}/'.format(page)\n",
    "    respone = requests.get(url)\n",
    "    rawhtml = respone.text\n",
    "    soup = BeautifulSoup(rawhtml, \"html.parser\")\n",
    "    home = soup.find_all(\"div\",\"elementor-post__card\")\n",
    "    for i in home:\n",
    "        judul = i.find(\"h3\",\"elementor-post__title\").text.strip()\n",
    "        tanggal = i.find(\"span\",\"elementor-post-date\").text.strip()\n",
    "        list.append([judul,tanggal])\n",
    "writer = csv.writer(open('example.csv', 'w', newline=\"\"))\n",
    "for i in list:\n",
    "    writer.writerow(i)\n",
    "writer = csv.writer(open('example.csv', 'w', newline=\"\"))\n",
    "for i in list:\n",
    "    writer.writerow(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tahap pertama adalah membuat list kosong dengan maxpage adalah 20 (artinya jumlah slide yang akan discraping ada sampai 20 halaman, sehingga diberlakukan looping supaya dapat terscrapping perhalaman). Lalu looping (for) dari rentang 1 sampai 21 (maxpage) dan setelah itu soup dilakukan untuk scraw;ing dari looping setiap judul (dengan tag di inspect h3 dan class elementor-post__tittle) dan tanggal terbit (dengan tag span dan class elementor-post-date). Lalu setiap hasil scrawling ditambahkan ke variabel list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = csv.writer(open('example.csv', 'w', newline=\"\"))\n",
    "for i in list:\n",
    "    writer.writerow(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Di atas adalah untuk membuat menjadi csv dengan library writer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nomor 3\n",
    "Untuk nomor 3 kita bisa gunakan scrapy dengan command prompt lalu membuat project baru dengan scrapy startproject dengan nama folder nomor 3. lalu untuk load, kita buka file python baru dengan mengetik command genspider nomor 3 dan link dari tempat penyimpangan folder lokal. Setelah itu kita bisa akses codenya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m<tokenize>:6\u001b[1;36m\u001b[0m\n\u001b[1;33m    name = 'nomor3'\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "class CobaSpider(scrapy.Spider):\n",
    "    name = 'nomor3'\n",
    "    start_urls = [\"https://store.playstation.com/en-id/category/805ff1ce-4e9f-4b73-8508-7cd344c40afb/\"]\n",
    "    \n",
    "\n",
    "  name = 'nomor3'\n",
    "    start_urls = ['https://store.playstation.com/en-id/category/805ff1ce-4e9f-4b73-8508-7cd344c40afb/']    \n",
    "\n",
    "    def parse(self, response):\n",
    "        url = response.url\n",
    "        for i in range(1,17):\n",
    "            yield scrapy.Request(url=url+str(i), callback=self.parse_details)\n",
    "    def parse_details(self, response):\n",
    "        for text in response.css(\".psw-product-tile__details\"):\n",
    "            yield{\n",
    "                \"Harga Diskon\":text.css(\".psw-m-r-3::text\").get()\n",
    "            }\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada tahap ini kita load url dibaca, saya menggunakan fungsi dengan variabel self dan response. Dimana fungsi akan mendeklarasikan halaman. Selanjutnya kita definisikan halaman yg diambil untuk crawling dari span.psw-fill dan diubah menjadi integer. Setelah itu kita lakukan looping dengan for dari slide 1 sampai dengan 17 untuk scrawling dengan scrapy dengan def perse. Setelah itu kita lakukan pencaharian harga diskon dengan class psw-m-r-3::text. Tahap selanjutnya kita lakukan di terminal dengan run diketik sebagai scrapy crawl <nama_spider> -t json -o <nama_file_output>.json Setelah dienter, maka output akan muncul dengan dokumen json."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link github dapat diakses di : https://github.com/thetheresa/scraping_scrawling.git"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "80e894ad95d1f4379de15280113bbf668f7a81ce9700944b2c921d9fe356514f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
